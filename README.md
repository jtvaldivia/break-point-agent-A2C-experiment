# 游 Trauma en Aprendizaje por Refuerzo (RL) con A2C en Breakout

Este proyecto explora el **efecto de eventos traum치ticos en el comportamiento de un agente de RL**, usando el juego **Breakout** y el algoritmo **A2C (Advantage Actor-Critic)** como base de entrenamiento.

## 游꿢 Objetivo

Simular c칩mo un **evento traum치tico 칰nico** (castigo fuerte por romper un bloque) puede afectar el comportamiento a largo plazo de un agente. Compararemos dos agentes entrenados:

- 游릭 **Agente sin trauma**: Juega normalmente.
- 游댮 **Agente con trauma**: Recibe un gran castigo al romper un bloque en el primer cuarto del episodio.

Queremos observar:

- C칩mo cambia el comportamiento del agente traumatizado.
- Si evita la acci칩n castigada.
- Qu칠 tanto se recupera con el tiempo.

---

## 游빍 쯇or qu칠 este experimento?

Inspirado por fen칩menos humanos como el trauma y el aprendizaje aversivo, este experimento busca evaluar c칩mo **un solo evento negativo significativo** puede alterar decisiones futuras en agentes de RL, especialmente en contextos donde se espera maximizar la recompensa.

---

## 丘뙖잺 Estructura del C칩digo

| Archivo                      | Descripci칩n                                                                 |
|-----------------------------|-----------------------------------------------------------------------------|
| `a2c_breakout_trauma.py`     | C칩digo principal que entrena ambos agentes (con y sin trauma) y guarda modelos. |
| `view_agents.py`             | Visualiza el comportamiento de los agentes entrenados.                     |
| `trauma_wrapper.py`          | Define el wrapper que aplica el trauma al agente.                          |

---

## 郊윒잺 C칩mo ejecutar

### 1. Entrenar ambos agentes
```bash
python a2c_breakout_trauma.py
```
Esto entrenar치 dos agentes (con y sin trauma) y guardar치 los modelos.


### 2. Visualizar los agentes
```bash
python view_agents.py --specific N

```
Muestra el episodio n칰mero N para ambos agentes.

### 3. Ver Todos los episodios

```bash
python view_agents.py

```
Reproduce todos los episodios guardados de cada agente.

## 游늵 쯈u칠 analizamos?

- **Comportamiento post-trauma:** 쮼l agente evita romper bloques temprano luego del castigo?
- **Capacidad de recuperaci칩n:** 쮼l agente vuelve a intentar la acci칩n castigada si ve que es beneficiosa a largo plazo?
- **Comparaci칩n cualitativa:** A trav칠s de los videos de episodios se pueden observar diferencias visibles en la estrategia adoptada.
- **Rol del algoritmo:** A2C es on-policy, lo que lo hace m치s susceptible a evitar caminos castigados, aunque sean 칩ptimos.

---

## 游 Conclusiones esperadas

- **El trauma introduce un sesgo** en el aprendizaje del agente, especialmente si ocurre en fases tempranas de entrenamiento.
- **Agentes on-policy como A2C** tienden a volverse m치s conservadores ante castigos fuertes, evitando caminos que pueden ser 칩ptimos.
- El experimento busca evidenciar que incluso **una 칰nica experiencia negativa** puede tener un impacto duradero y observable.
- Esto permite discutir paralelos con el aprendizaje humano, donde eventos traum치ticos pueden moldear fuertemente la conducta futura.

---

## 游닍 Requisitos

- Python 3.9+
- `gymnasium[atari]`
- `stable-baselines3`
- `matplotlib`
- `opencv-python` (si se usa grabaci칩n de video con OpenCV)

Instalaci칩n recomendada:
```bash
pip install stable-baselines3[extra] gymnasium[atari] matplotlib opencv-python
```

